\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{fancyhdr}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}

\usepackage{listings}
\usepackage{color}
\lstset{language=Python,
        basicstyle=\footnotesize\ttfamily,
        showspaces=false,
        showstringspaces=false,
        tabsize=2,
        breaklines=false,
        breakatwhitespace=true,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
    }

\usepackage[pdftex]{graphicx}

% header
\fancyhead{}
\fancyfoot{}
\fancyfoot[C]{\thepage}
\fancyhead[R]{Daniel Foreman-Mackey}
\fancyhead[L]{Statistical Natural Language Processing --- Homework 2}
\pagestyle{fancy}
\setlength{\headsep}{10pt}
\setlength{\headheight}{20pt}

% shortcuts
\newcommand{\Eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eq}[1]{Equation (\ref{eq:#1})}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}

\newcommand{\pr}[1]{\ensuremath{p\left (#1 \right )}}
\newcommand{\lk}[1]{\ensuremath{\mathcal{L} \left ( #1 \right )}}
\newcommand{\bvec}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\dd}{\ensuremath{\, \mathrm{d}}}
\newcommand{\normal}[2]{\ensuremath{\mathcal{N} \left ( #1; #2 \right ) }}
\newcommand{\T}{^\mathrm{T}}

\newcommand{\data}{\mathcal{D}}
\newcommand{\code}[1]{{\sffamily #1}}


\begin{document}

The goal of this project is to classify a noun into one of five categories
(\code{place}, \code{movie}, \code{drug}, \code{person}, or \code{company})
using only the word itself.
I'll focus on building a discriminative model based on logistic regression.

\section{A note about implementation}

Instead of using the provided Java code, I decided to implement my assignment
in Python and C.
The data manipulation and feature extraction is all performed in Python using
the standard library.
Once the feature vectors have been built, they are passed to the C code that
does the computationally heavy lifting.
The standard Python implementation of \code{BFGS} was not efficient enough for
the purposes of this assignment so I used the \code{libLBFGS}%
\footnote{\url{http://www.chokkan.org/software/liblbfgs/}} C implementation
of the \code{L-BFGS} algorithm.
All of the code used in this assignment is available on GitHub at:
\url{https://github.com/dfm/stat-nlp-nyu}.

\section{General description of logistic model}

The basic model that I'm going to focus on is a discriminative logistic
classifier.
This model has the form
\begin{eqnarray}
p(y_i\,|\,\bvec{x}_i,\,\bvec{w}) &=&
\frac{\exp\left[\bvec{w}_{y_i}\T\cdot\bvec{f}(\bvec{x}_i)\right]}
{\sum_{y^\prime}
\exp\left[\bvec{w}_{y^\prime}\T\cdot\bvec{f}(\bvec{x}_i)\right)]}
\end{eqnarray}
where $\bvec{w}_{y_i}$ is the weight vector for class $y_i$ and $\bvec{f}$ is
an operator on the word $\bvec{x}_i$ that extracts a feature vector (the same
length as $\bvec{w}_y$) describing the ``important'' characteristics of the
word.
The choice of $\bvec{f}$ proves to be the hardest part of this assignment.

We want to find the weight vectors $\bvec{W}\equiv\{\bvec{w}_y\}$
that maximize the probability of the full training set
$\bvec{X}\equiv\{\bvec{x}_i\}$ (assuming independent examples)
\begin{eqnarray}
p(\{y_i\}\,|\,\bvec{X},\,\bvec{W}) &=&
\prod_i p(y_i\,|\,\bvec{x}_i,\,\bvec{w})\quad.
\end{eqnarray}
In practice, most optimization algorithms are designed to minimize a function
so we'll actually be computing the \emph{negative log-probability}
\begin{eqnarray}
\ell(\bvec{W}) &=&
-\sum_i \ln p(y_i\,|\,\bvec{x}_i,\,\bvec{w}) \nonumber\\
&=&
\sum_{i} \left [
\ln\left (\sum_{y^\prime}
\exp\left[\bvec{w}_{y^\prime}\T\cdot\bvec{f}(\bvec{x}_i)\right)]\right)
- \bvec{w}_{y_i}\T\cdot\bvec{f}(\bvec{x}_i) \right ]
\label{eq:logistic}
\end{eqnarray}

\paragraph{The gradient of equation~\ref{eq:logistic}}
I'll be using a gradient descent algorithm to find the maximum a posteriori
parameters for the probabilistic model given by equation~\ref{eq:logistic} so
I'll need to compute the gradient of that function.
Symbolically, this works out to be
\begin{eqnarray}
\frac{\dd \ell(\bvec{W})}{\dd w_{y,\,k}} &=&
\sum_{i}\left[p(y_i\,|\,\bvec{x}_i,\,\bvec{w}_{y_i})
- \bvec{1}(y_i = y) \right]\,f_k(\bvec{x}_i) \quad.
\end{eqnarray}
I made sure the check my implementation numerically as well using centered
finite difference.

\paragraph{Regularization}
To avoid over fitting, I also include the standard L2 regularization term
\begin{eqnarray}
\label{eq:regularization}
\ell_\mathrm{L2} (\bvec{W}) &=&
-\frac{1}{2\,\sigma^2}\,||\,\bvec{W}\,||_2^2
\end{eqnarray}
and optimize the sum of equations~\ref{eq:logistic}
and~\ref{eq:regularization}
\begin{eqnarray}
\label{eq:fullmodel}
\ell^\prime (\bvec{W}) = \ell (\bvec{W}) + \ell_\mathrm{L2} (\bvec{W}) \quad.
\end{eqnarray}
I also included the relevant terms in the gradient and confirmed that they
were correctly computed numerically.
I will discuss the choice of the free parameter $\sigma$ below.

\paragraph{Extractor implementation}
The classifier that I train in this assignment takes a list of feature
extractors and concatenates the results into the full feature vector.
This means that arbitrary combinations of extractors can be easily combined at
run time to invoke different experiments.
To start with, I'll describe and show the results for a simple
\code{UnigramExtractor} model.

\section{Unigram discriminative model}

The first model that I experimented with was a unigram model.
To extract the features for this model, I simply count the number of times
each character occurs in the string to build a character histogram.
Using these features and the probability model given by
equation~\ref{eq:fullmodel}, I trained the weights using \code{L-BFGS} on the
provided training set of examples.
For this model, I simply used the default regularization with $\sigma = 1$.
To test for convergence---and as an attempt to avoid over fitting---I plotted
the probability and train/validation set performance as a function of
the number of optimization iterations in \fig{unigram-convergence}.
These features are clearly not very effective for classification producing a
(seemingly converged) validation set accuracy of only $\sim67\%$.
The intriguing thing about this plot is that the validation accuracy is
actually \emph{better} than the training accuracy.
This is surprising and I didn't see this behavior with any of the other
feature sets that I tried.
My guess is since the unigram counts are not a very descriptive feature, we
simply got lucky with the validation set and the noise in the predictions
pushed the accuracy up artificially.

To explore the performance of my model, I also plotted the confusion matrix on
the validation set in \fig{unigram-confusion}.
This figure shows the fraction of validation samples classified as some class
\code{guess} (columns) when the true label is \code{gold} (rows).
This figure isn't particularly instructive in this case but we can already see
that the main confusion is between \code{person} and \code{movie}.
This isn't too surprising given that it's not uncommon for a movie to actually
be named after a person and when I look at the data, there are a few
surprising classifications!

In order to check that the estimated confidence of the model predictions are
reasonable, I looked at the validation accuracy as a function of confidence.
\Fig{unigram-confidence-hist} shows the total number of correctly and
incorrectly classified validation samples in confidence bins.
As expected, the distribution of correctly classified samples peaks near high
confidence and the number of incorrectly classified samples is negligible when
the confidence is $\gtrsim 80 \%$.
\Fig{unigram-confidence-scale} is even more instructive.
This figure show the fraction of correctly classified samples in each of the
bins as an estimate of the empirical confidence of the classifications.
If the confidence values returned by the model were correct, this figure
should show a one-to-one correlation.
Instead, this model seems to be slightly \emph{under-confident} since most of
the points are shifted a bit left of the one-to-one line.

\paragraph{Note on capitalization}
I decided to use the full \emph{case-sensitive} histogram because the number
(and identities) of the capitalized letters seemed like they would be good
features (especially for distinguishing \code{drug}s from other labels).
I also found that in practice the model had better performance when using the
capitalized characters but it's possible that we could do just as well using
some sort of indicator feature that counts the number of capitalized
characters.

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=\textwidth]{unigram_convergence.pdf}
\end{center}
\caption{%
The progress of the optimization of equation~\ref{eq:logistic} as a function
of number of iterations.
\emph{top:} The value of equation~\ref{eq:fullmodel}.
\emph{bottom:} The predictive accuracy of the model on the validation and test
sets (as labeled).
\figlabel{unigram-convergence}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=\textwidth]{unigram_confusion.pdf}
\end{center}
\caption{%
The confusion matrix resulting from training $\bvec{W}$ in
equation~\ref{eq:fullmodel} using only unigram features of the training set
and then predicting the validation labels.
The cells indicate the fractional number of samples classified as \code{guess}
conditioned on their ``true'' label \code{gold}.
\figlabel{unigram-confusion}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.6\textwidth]{unigram_confidence_hist.pdf}
\end{center}
\caption{%
The distribution of correctly (thick solid histogram) and incorrectly (thin
dashed histogram) classified samples in the validation set (using the
unigram model) as a function of the model's predicted confidence.
\figlabel{unigram-confidence-hist}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.6\textwidth]{unigram_confidence_scale.pdf}
\end{center}
\caption{%
The fraction of correctly classified samples in the validation set (using the
unigram model) as a function of model confidence.
The light grey line shows the one-to-one relation that the samples should
follow if the confidences were exactly correct.
\figlabel{unigram-confidence-scale}}
\end{figure}

\clearpage

\section{Larger N-gram models}

Since the unigram features were clearly not sufficiently discriminative to be
good for classification, the next feature set that I decided to try was:
\code{UnigramExtractor + BigramExtractor}.
The \code{BigramExtractor} is similar to \code{UnigramExtractor} but it counts
two-character pairs.
To also take into account the suffix and prefix characters of the strings, I
appended a \code{STOP} character and prepended a \code{START} character to the
string before counting the pairs.
At first, I tried just appending and prepending spaces but I found that the
performance was better if I treated the string boundaries as different than
word boundaries.
This seems especially reasonable for samples labeled \code{company} because
it is the only class that regularly ends with a period.

\Fig{bigram-convergence} shows the convergence of the model as a function of
optimization steps.
This model does somewhat better than the unigram features alone---as one would
hope, given how much more flexible the model is---but the features are clearly
still not discriminative enough (only $\sim 82.5\%$ accuracy on the validation
set) to be useful in practice.
\Fig{bigram-confusion} shows the confusion matrix for this feature set and the
most striking feature is the confusion between the \code{person} and
\code{movie} labels.
Taking a look at the validation errors, it's interesting to note that the
token \code{Ben Affleck}, for example, is labeled as a \code{movie} when any
human could easily guess that he's probably a \code{person}.
I'll keep this---and other similar examples---in mind in the next section.

I also implemented a trigram feature extractor and the results again get
marginally better: converging to $\sim 88\%$ accuracy on the validation set
(using unigrams, bigrams, and trigrams).
It's interesting to note here that in all of these examples, the performance
of my models on the test set was consistently and systematically less accurate
by $\sim 1\%$ than on the validation set.
\Fig{trigram-confusion} shows the confusion matrix produced by this feature
set.
In this model, most of the confusion is between \code{place} and \code{movie}.

It is also interesting to note (from \fig{trigram-confidence-scale}) that this
model is somewhat \emph{over-confident} in its predictions.
To attempt to correct for this, I tried optimizing this model with a
different value of the L2 normalization scale $\sigma$.
In production, it would be a good idea to run a grid of models with different
values of $\sigma$ and chose the best one using cross-validation.
Since I didn't have time---or the computational resources---to try this for
this assignment, I simply tried a second value of $\sigma = 1/2$.
This produces somewhat better predictions (see \fig{trigram-0.5-confusion})
and the confidences were much better calibrated (see
\fig{trigram-0.5-confidence-scale}).
This is probably because the initial trigram model was over-fitting the
training data.

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{bigrams_convergence.pdf}
\end{center}
\caption{%
The same a \fig{unigram-convergence} but including bigram features.
\figlabel{bigram-convergence}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{bigrams_confusion.pdf}
\end{center}
\caption{%
The same a \fig{unigram-confusion} but including bigram features.
\figlabel{bigram-confusion}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{trigram_confusion.pdf}
\end{center}
\caption{%
The same a \fig{unigram-confusion} but including bigram and trigram
features.
\figlabel{trigram-confusion}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{trigram_confidence_scale.pdf}
\end{center}
\caption{%
The same a \fig{unigram-confidence-scale} but including bigram and trigram
features.
\figlabel{trigram-confidence-scale}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{trigram_0_5_confusion.pdf}
\end{center}
\caption{%
The same a \fig{trigram-confusion} but with $\sigma = 1/2$.
\figlabel{trigram-0.5-confusion}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{trigram_0_5_confidence_scale.pdf}
\end{center}
\caption{%
The same a \fig{trigram-confidence-scale} but with $\sigma = 1/2$.
\figlabel{trigram-0.5-confidence-scale}}
\end{figure}

\clearpage

\section{Descriptive features}

After trying the default N-gram feature sets described above, I wanted to try
some more ``descriptive'' features based on something like \emph{domain
knowledge} (translation: looking at the validation set and guessing).
Some examples of features that I thought of are:
\begin{itemize}
\item{the total number of characters in the string,}
\item{the number of words in the string,}
\item{the number of digits in the string,}
\item{the number and identities of the stop words in the string (English,
      French, etc.),}
\item{the N-character suffixes and prefixes (considering the individual
      words or the full string), and}
\item{the number of words in the string that match the list of popular baby
      names in the United States%
\footnote{\url{http://www.ssa.gov/oact/babynames/}}.}
\end{itemize}
(I never actually implemented that last one but it does seem like it could be
useful!)
While debugging, I ran many combinations of these features to get an intuition
for which features were most useful.
In practice, the gains to be made over the basic trigram model seem to be
pretty small and a huge number of parameters---in other words: many
computations---are required to make a small difference in accuracy.
As a demonstration, consider the model built on the features: unigrams,
bigrams, trigrams, number of characters, number of words, number of digits,
3-suffixes, 4-suffixes, 3-prefixes, and 4-prefixes.
This model has many more parameters than the trigram model but the accuracy
(shown in \fig{insane-0.5-confusion}) is only moderately better than the
previous model.

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{final/insane_0_5_confusion.pdf}
\end{center}
\caption{%
The same a \fig{trigram-0.5-confusion} but with the features: unigrams,
bigrams, trigrams, number of characters, number of words, number of digits,
3-suffixes, 4-suffixes, 3-prefixes, and 4-prefixes.
\figlabel{insane-0.5-confusion}}
\end{figure}

\clearpage

\section{Empirical N-gram models}

For comparison, I used the code that wrote for homework 1 (before I realized
that all of the solutions were mistakenly provided) to generate a set of
empirical N-gram probability distributions for each class.
Given these distributions, the posterior probability of the class $y$
conditioned on the word $\bvec{x}$ is
\begin{eqnarray}
p(y\,|\,\bvec{x}) &\propto& p(y)\,p(\bvec{x}\,|\,y)
\end{eqnarray}
where $p(y)$ is the empirical prior probability of observing the class $y$ and
\begin{eqnarray}
p(\bvec{x}\,|\,y) &=& \prod_{n} p(x_n\,|\,x_{n-1},\,x_{n-2},\,\ldots) \quad.
\end{eqnarray}
Each term in the likelihood is given by the empirical distribution of N-grams
in the training set.

Applying this model na\"ively using only character unigrams gives an accuracy
of only $50.5\%$ on the validation set.
For comparison, I got the best performance ($78.2\%$ accuracy on the
validation set) by using a linearly interpolated trigram model:
\begin{eqnarray}
p(\bvec{x}\,|\,y) &=& \lambda_3\,p_\mathrm{tri}(\bvec{x}\,|\,y)
+(1-\lambda_3)\,\left [\lambda_2\,p_\mathrm{bi}(\bvec{x}\,|\,y)
+(1-\lambda_2)\,p_\mathrm{uni}(\bvec{x}\,|\,y)\right]
\end{eqnarray}
where $p_\mathrm{tri}$ is the standard empirical trigram conditional
likelihood, and $p_\mathrm{bi}$ and $p_\mathrm{uni}$ are the same for bigrams
and unigrams.
I found the most accurate training results with $\lambda_2=0.9$ and
$\lambda_3=0.9$.

The confusion matrix for this model is shown in \fig{empirical-confusion} and
most of the error features seen in the previous sections are visible (and more
pronounced).
Comparing \fig{empirical-confidence-scale} to \fig{trigram-confidence-scale},
we can see that the empirical model is even more over-confident than the basic
trigram model (even before regularization).

The accuracy of this method is far worse than the discriminative model that
uses the same features ($\sim 88\%$ accuracy for the discriminative model
using unigram, bigram and trigram features) but it is far less computationally
expensive to train.
The performance is worse because the discriminative models are specifically
designed to upweight the features that are good at \emph{discriminating}
between classes versus these empirical models that basically treat each
example of a feature with equal weight.
For the problem that we're approaching in this assignment, a discriminative
model is probably more appropriate because all we care about is how well we
can determine that labels of the words.
If we were asking a different question (like, for example, trying to better
understand the structure of language and why particular character
distributions are associated with particular classes of nouns) then a
generative approach might be better because you can apply structure to the
model and prior information (motivated domain knowledge and the question at
hand) making the results more interpretable.

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{empirical_confusion.pdf}
\end{center}
\caption{%
The same a \fig{unigram-confusion} but for the empirical trigram model
described in the text.
\figlabel{empirical-confusion}}
\end{figure}

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=0.65\textwidth]{empirical_confidence_scale.pdf}
\end{center}
\caption{%
The same a \fig{unigram-confidence-scale} but for the empirical trigram model
described in the text.
\figlabel{empirical-confidence-scale}}
\end{figure}

\end{document}
